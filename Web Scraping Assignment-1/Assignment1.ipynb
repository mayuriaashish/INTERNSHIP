{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa0f6043",
   "metadata": {},
   "source": [
    "# Question 1 wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8ac2b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65306222",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4571f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "081566bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_tags = []\n",
    "for tag in range(1, 7):\n",
    "    headers = soup.find_all('h{}'.format(tag))\n",
    "    for header in headers:\n",
    "        header_tags.append(header.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35fd4859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Header\n",
      "0                       Main Page\n",
      "1            Welcome to Wikipedia\n",
      "2   From today's featured article\n",
      "3                Did you know ...\n",
      "4                     In the news\n",
      "5                     On this day\n",
      "6      From today's featured list\n",
      "7        Today's featured picture\n",
      "8        Other areas of Wikipedia\n",
      "9     Wikipedia's sister projects\n",
      "10            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "header_df = pd.DataFrame(header_tags, columns=['Header'])\n",
    "print(header_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b188d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028981e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca657770",
   "metadata": {},
   "source": [
    "# Question 2 imdb top rated movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10eaed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f55fdaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a0492a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e67c1290",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5cd18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = soup.select('td.titleColumn')\n",
    "ratings = soup.select('td.ratingColumn.imdbRating')\n",
    "movie_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09e7f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(movies)):\n",
    "    name = movies[i].find('a').text\n",
    "    year = movies[i].find('span').text.strip('()')\n",
    "    rating = ratings[i].text.strip()\n",
    "    movie_data.append({'Name': name, 'Year': year, 'Rating': rating})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "19d73a8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Name  Year Rating\n",
      "0                            The Shawshank Redemption  1994    9.2\n",
      "1                                       The Godfather  1972    9.2\n",
      "2                                     The Dark Knight  2008    9.0\n",
      "3                               The Godfather Part II  1974    9.0\n",
      "4                                        12 Angry Men  1957    9.0\n",
      "5                                    Schindler's List  1993    8.9\n",
      "6       The Lord of the Rings: The Return of the King  2003    8.9\n",
      "7                                        Pulp Fiction  1994    8.8\n",
      "8   The Lord of the Rings: The Fellowship of the Ring  2001    8.8\n",
      "9                     Il buono, il brutto, il cattivo  1966    8.8\n",
      "10                                       Forrest Gump  1994    8.8\n",
      "11                                         Fight Club  1999    8.7\n",
      "12              The Lord of the Rings: The Two Towers  2002    8.7\n",
      "13                                          Inception  2010    8.7\n",
      "14                            The Empire Strikes Back  1980    8.7\n",
      "15                                         The Matrix  1999    8.7\n",
      "16                                         GoodFellas  1990    8.7\n",
      "17                    One Flew Over the Cuckoo's Nest  1975    8.6\n",
      "18                                              Se7en  1995    8.6\n",
      "19                               Shichinin no samurai  1954    8.6\n",
      "20                              It's a Wonderful Life  1946    8.6\n",
      "21                           The Silence of the Lambs  1991    8.6\n",
      "22                                Saving Private Ryan  1998    8.6\n",
      "23                                     Cidade de Deus  2002    8.6\n",
      "24                                       Interstellar  2014    8.6\n",
      "25                                    La vita è bella  1997    8.6\n",
      "26                                     The Green Mile  1999    8.6\n",
      "27                                          Star Wars  1977    8.5\n",
      "28                         Terminator 2: Judgment Day  1991    8.5\n",
      "29                                 Back to the Future  1985    8.5\n",
      "30                      Sen to Chihiro no kamikakushi  2001    8.5\n",
      "31                                        The Pianist  2002    8.5\n",
      "32                                             Psycho  1960    8.5\n",
      "33                                       Gisaengchung  2019    8.5\n",
      "34                                               Léon  1994    8.5\n",
      "35                                      The Lion King  1994    8.5\n",
      "36                                          Gladiator  2000    8.5\n",
      "37                                 American History X  1998    8.5\n",
      "38                                       The Departed  2006    8.5\n",
      "39                                 The Usual Suspects  1995    8.5\n",
      "40                                       The Prestige  2006    8.5\n",
      "41                                           Whiplash  2014    8.5\n",
      "42                                         Casablanca  1942    8.5\n",
      "43                                     Hotaru no haka  1988    8.5\n",
      "44                                            Seppuku  1962    8.5\n",
      "45                                   The Intouchables  2011    8.5\n",
      "46                                       Modern Times  1936    8.4\n",
      "47                       Once Upon a Time in the West  1968    8.4\n",
      "48                                        Rear Window  1954    8.4\n",
      "49                              Nuovo Cinema Paradiso  1988    8.4\n"
     ]
    }
   ],
   "source": [
    "movie_df = pd.DataFrame(movie_data)\n",
    "print(movie_df.head(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09024467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03fc087f",
   "metadata": {},
   "source": [
    "# Question 3 indian movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a900007f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Name  Rating  Year\n",
      "0   Ramayana: The Legend of Prince Rama     8.5  1993\n",
      "1            Rocketry: The Nambi Effect     8.4  2022\n",
      "2                               Nayakan     8.4  1987\n",
      "3                              Gol Maal     8.4  1979\n",
      "4                            Anbe Sivam     8.4  2003\n",
      "5                           777 Charlie     8.4  2022\n",
      "6                              Jai Bhim     8.4  2021\n",
      "7                     Pariyerum Perumal     8.4  2018\n",
      "8                              3 Idiots     8.4  2009\n",
      "9                           Apur Sansar     8.4  1959\n",
      "10                     Manichitrathazhu     8.3  1993\n",
      "11                                #Home     8.3  2021\n",
      "12                      Soorarai Pottru     8.3  2020\n",
      "13                         Black Friday     8.3  2004\n",
      "14                    Kumbalangi Nights     8.3  2019\n",
      "15                    C/o Kancharapalem     8.3  2018\n",
      "16                     Taare Zameen Par     8.3  2007\n",
      "17                             Kireedam     8.3  1989\n",
      "18                               Dangal     8.3  2016\n",
      "19                               Kaithi     8.3  2019\n",
      "20                               Jersey     8.3  2019\n",
      "21                                   96     8.3  2018\n",
      "22                          Maya Bazaar     8.2  1957\n",
      "23                            Natsamrat     8.2  2016\n",
      "24                               Asuran     8.2  2019\n",
      "25                           Drishyam 2     8.2  2021\n",
      "26                           Sita Ramam     8.2  2022\n",
      "27                         Thevar Magan     8.2  1992\n",
      "28                           Visaaranai     8.2  2015\n",
      "29                  Sarpatta Parambarai     8.2  2021\n",
      "30                           Thalapathi     8.2  1991\n",
      "31                      Pather Panchali     8.2  1955\n",
      "32                         Nadodikkattu     8.2  1987\n",
      "33                             Drishyam     8.2  2013\n",
      "34                   Jaane Bhi Do Yaaro     8.2  1983\n",
      "35                         Thani Oruvan     8.2  2015\n",
      "36                         Sardar Udham     8.2  2021\n",
      "37                            Aparajito     8.2  1956\n",
      "38                         Vada Chennai     8.2  2018\n",
      "39                    Khosla Ka Ghosla!     8.2  2006\n",
      "40                              Anniyan     8.1  2005\n",
      "41                             Ratsasan     8.1  2018\n",
      "42                        Chupke Chupke     8.1  1975\n",
      "43                   Gangs of Wasseypur     8.1  2012\n",
      "44                              Peranbu     8.1  2018\n",
      "45                             Drishyam     8.1  2015\n",
      "46                             Mahanati     8.1  2018\n",
      "47                       Bangalore Days     8.1  2014\n",
      "48                                Satya     8.1  1998\n",
      "49                               Premam     8.1  2015\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.imdb.com/india/top-rated-indian-movies/'\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "table = soup.find('tbody', {'class': 'lister-list'})\n",
    "\n",
    "\n",
    "names = []\n",
    "ratings = []\n",
    "years = []\n",
    "\n",
    "\n",
    "for row in table.find_all('tr'):\n",
    "\n",
    "    name = row.find('td', {'class': 'titleColumn'}).find('a').text\n",
    "    names.append(name)\n",
    "    \n",
    "    \n",
    "    rating = row.find('td', {'class': 'ratingColumn'}).find('strong').text\n",
    "    ratings.append(float(rating))\n",
    "    \n",
    "    \n",
    "    year = row.find('td', {'class': 'titleColumn'}).find('span', {'class': 'secondaryInfo'}).text\n",
    "    years.append(year.strip('()'))\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Name': names, 'Rating': ratings, 'Year': years})\n",
    "\n",
    "\n",
    "print(df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130f732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37f38e71",
   "metadata": {},
   "source": [
    "# question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aabfa706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "                 President Name                        Term of Office\n",
      "0          Shri Ram Nath Kovind       25 July, 2017 to 25 July, 2022 \n",
      "1         Shri Pranab Mukherjee       25 July, 2012 to 25 July, 2017 \n",
      "2  Smt Pratibha Devisingh Patil       25 July, 2007 to 25 July, 2012 \n",
      "3        DR. A.P.J. Abdul Kalam       25 July, 2002 to 25 July, 2007 \n",
      "4          Shri K. R. Narayanan       25 July, 1997 to 25 July, 2002 \n",
      "5       Dr Shankar Dayal Sharma       25 July, 1992 to 25 July, 1997 \n",
      "6           Shri R Venkataraman       25 July, 1987 to 25 July, 1992 \n",
      "7              Giani Zail Singh       25 July, 1982 to 25 July, 1987 \n",
      "8     Shri Neelam Sanjiva Reddy       25 July, 1977 to 25 July, 1982 \n",
      "9      Dr. Fakhruddin Ali Ahmed  24 August, 1974 to 11 February, 1977\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "response = requests.get(url)\n",
    "print(response)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "table = soup.find('ul', {'class': 'listing cf'})\n",
    "\n",
    "names = []\n",
    "terms = []\n",
    "\n",
    "for row in table.findAll('li') :\n",
    "    name = row.find('h3').text\n",
    "    finalName = name.split(\"(\")[0].strip()\n",
    "    names.append(finalName)\n",
    "\n",
    "    termOfOffice = row.find('p').text\n",
    "    termOfOfficeFinal = termOfOffice.replace(\"Term of Office: \", \"\")\n",
    "    terms.append(termOfOfficeFinal)\n",
    "\n",
    "df = pd.DataFrame({'President Name': names, 'Term of Office': terms})\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e847000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6cf3fcc",
   "metadata": {},
   "source": [
    "# question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3267cbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0        India\\nIND      44  5,010    114\n",
      "1    Australia\\nAUS      32  3,572    112\n",
      "2   New Zealand\\nNZ      29  3,229    111\n",
      "3      England\\nENG      33  3,656    111\n",
      "4     Pakistan\\nPAK      25  2,649    106\n",
      "5  South Africa\\nSA      27  2,775    103\n",
      "6   Bangladesh\\nBAN      33  3,129     95\n",
      "7     Sri Lanka\\nSL      34  2,976     88\n",
      "8  Afghanistan\\nAFG      20  1,419     71\n",
      "9   West Indies\\nWI      41  2,902     71\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "teams = []\n",
    "matches = []\n",
    "points = []\n",
    "ratings = []\n",
    "\n",
    "\n",
    "for row in rows[1:11]:\n",
    "\n",
    "    team = row.find_all('td')[1].text.strip()\n",
    "    teams.append(team)\n",
    "    \n",
    "    \n",
    "    match = row.find_all('td')[2].text.strip()\n",
    "    matches.append(match)\n",
    "    \n",
    "    \n",
    "    point = row.find_all('td')[3].text.strip()\n",
    "    points.append(point)\n",
    "\n",
    "    rating = row.find_all('td')[4].text.strip()\n",
    "    ratings.append(rating)\n",
    "df = pd.DataFrame({'Team': teams, 'Matches': matches, 'Points': points, 'Rating': ratings})\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40edfb15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "330cd48d",
   "metadata": {},
   "source": [
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c4013e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Batsman Team Rating\n",
      "0  Rassie van der Dussen   SA    787\n",
      "1           David Warner  AUS    747\n",
      "2        Quinton de Kock   SA    743\n",
      "3            Imam-ul-Haq  PAK    740\n",
      "4           Shubman Gill  IND    734\n",
      "5            Virat Kohli  IND    727\n",
      "6            Steve Smith  AUS    719\n",
      "7           Rohit Sharma  IND    719\n",
      "8        Kane Williamson   NZ    700\n",
      "9           Fakhar Zaman  PAK    699\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "batsmen = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "for row in soup.select('tr.table-body'):\n",
    "    cells = row.select('td')\n",
    "    batsman = cells[1].get_text().strip()\n",
    "    team = cells[2].get_text().strip()\n",
    "    rating = cells[3].get_text().strip() \n",
    "    batsmen.append(batsman)\n",
    "    teams.append(team)\n",
    "    ratings.append(rating)\n",
    "\n",
    "data = {'Batsman': batsmen, 'Team': teams, 'Rating': ratings}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.head(10)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "978d5035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Bowler Team Rating\n",
      "0     Josh Hazlewood  AUS    727\n",
      "1        Trent Boult   NZ    708\n",
      "2     Mitchell Starc  AUS    665\n",
      "3        Rashid Khan  AFG    659\n",
      "4         Adam Zampa  AUS    655\n",
      "5    Shakib Al Hasan  BAN    652\n",
      "6     Shaheen Afridi  PAK    641\n",
      "7  Mustafizur Rahman  BAN    638\n",
      "8   Mujeeb Ur Rahman  AFG    637\n",
      "9      Mohammad Nabi  AFG    631\n"
     ]
    }
   ],
   "source": [
    "#c\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "bowlers = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "for row in soup.select('tr.table-body'):\n",
    "    cells = row.select('td')\n",
    "    bowler = cells[1].get_text().strip()\n",
    "    team = cells[2].get_text().strip()\n",
    "    rating = cells[3].get_text().strip()\n",
    "    bowlers.append(bowler)\n",
    "    teams.append(team)\n",
    "    ratings.append(rating)\n",
    "\n",
    "data = {'Bowler': bowlers, 'Team': teams, 'Rating': ratings}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.head(10)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ef836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88187f58",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1bbfa951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      21  3,603    172\n",
      "1      England\\nENG      28  3,342    119\n",
      "2  South Africa\\nSA      26  3,098    119\n",
      "3        India\\nIND      27  2,820    104\n",
      "4   New Zealand\\nNZ      25  2,553    102\n",
      "5   West Indies\\nWI      27  2,535     94\n",
      "6   Bangladesh\\nBAN      13    983     76\n",
      "7     Thailand\\nTHA       8    572     72\n",
      "8     Pakistan\\nPAK      27  1,678     62\n",
      "9     Sri Lanka\\nSL       8    353     44\n"
     ]
    }
   ],
   "source": [
    "#a\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "teams = []\n",
    "matches = []\n",
    "points = []\n",
    "ratings = []\n",
    "\n",
    "\n",
    "for row in rows[1:11]:\n",
    "\n",
    "    team = row.find_all('td')[1].text.strip()\n",
    "    teams.append(team)\n",
    "    \n",
    "    \n",
    "    match = row.find_all('td')[2].text.strip()\n",
    "    matches.append(match)\n",
    "    \n",
    "    \n",
    "    point = row.find_all('td')[3].text.strip()\n",
    "    points.append(point)\n",
    "    \n",
    "    \n",
    "    rating = row.find_all('td')[4].text.strip()\n",
    "    ratings.append(rating)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Team': teams, 'Matches': matches, 'Points': points, 'Rating': ratings})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1731c54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5649a6f",
   "metadata": {},
   "source": [
    "#b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "909319b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Position               Player  \\\n",
      "0                                             1  (0)         Alyssa Healy   \n",
      "1                                             2  (0)          Beth Mooney   \n",
      "2                                             3  (0)      Laura Wolvaardt   \n",
      "3                                             4  (0)       Natalie Sciver   \n",
      "4  5  (2) This player has moved up in the ranking...          Meg Lanning   \n",
      "5  6  (1) This player has moved down in the ranki...     Harmanpreet Kaur   \n",
      "6  7  (1) This player has moved down in the ranki...      Smriti Mandhana   \n",
      "7                                             8  (0)       Rachael Haynes   \n",
      "8                                             9  (0)  Chamari Athapaththu   \n",
      "9                                            10  (0)    Amy Satterthwaite   \n",
      "\n",
      "  Team  Rating  \n",
      "0  AUS     762  \n",
      "1  AUS     754  \n",
      "2   SA     732  \n",
      "3  ENG     731  \n",
      "4  AUS     717  \n",
      "5  IND     716  \n",
      "6  IND     714  \n",
      "7  AUS     680  \n",
      "8   SL     655  \n",
      "9   NZ     641  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find_all('table')[0]\n",
    "\n",
    "df = pd.read_html(str(table))[0]\n",
    "\n",
    "df = df.head(10)[['Pos', 'Player', 'Team', 'Rating']]\n",
    "df.columns = ['Position', 'Player', 'Team', 'Rating']\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360321a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06e65822",
   "metadata": {},
   "source": [
    "#c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ea49782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Position            Player Team  Rating\n",
      "0   1  (0)   Hayley Matthews   WI     373\n",
      "1   2  (0)    Natalie Sciver  ENG     371\n",
      "2   3  (0)      Ellyse Perry  AUS     366\n",
      "3   4  (0)    Marizanne Kapp   SA     349\n",
      "4   5  (0)       Amelia Kerr   NZ     336\n",
      "5   6  (0)     Deepti Sharma  IND     322\n",
      "6   7  (0)  Ashleigh Gardner  AUS     292\n",
      "7   8  (0)     Jess Jonassen  AUS     250\n",
      "8   9  (0)          Nida Dar  PAK     232\n",
      "9  10  (0)    Jhulan Goswami  IND     214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find_all('table')[0]\n",
    "\n",
    "df = pd.read_html(str(table))[0]\n",
    "\n",
    "df = df.head(10)[['Pos', 'Player', 'Team', 'Rating']]\n",
    "df.columns = ['Position', 'Player', 'Team', 'Rating']\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1178c9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fa6c79f",
   "metadata": {},
   "source": [
    " # question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b3580308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline  Time  \\\n",
      "0   Stocks making the biggest moves after hours: D...  None   \n",
      "1   Stock futures slip Thursday night as inflation...  None   \n",
      "2   Dow closes 400 points lower as hot inflation r...  None   \n",
      "3   India is a true bright spot in the midst of a ...  None   \n",
      "4   Belarus' exiled opposition leader says trial h...  None   \n",
      "5   “I wouldn’t change my past if you gave me a tr...  None   \n",
      "6   Larry Summers says another Covid-scale problem...  None   \n",
      "7   Summers: 'Greatest tragedy' would be if centra...  None   \n",
      "8   Shell’s board of directors sued over climate s...  None   \n",
      "9   Bill Gates on why he'll carry on using private...  None   \n",
      "10  Why hydrogen made using nuclear may have a big...  None   \n",
      "11  'Greenwashing' is a good thing, according to o...  None   \n",
      "12  Concerns over golden eagles are partly prompti...  None   \n",
      "13  Supreme Court cancels arguments on Trump immig...  None   \n",
      "14  Biden says recently downed aerial objects were...  None   \n",
      "15  FDA advisors recommend over-the-counter use of...  None   \n",
      "16  Moderna's CEO will testify before Senate next ...  None   \n",
      "17  Biden outlines plans to cut U.S. deficit by $2...  None   \n",
      "18  Seen Big Ben already? Here's a Londoner's guid...  None   \n",
      "19  Shut out from their top destinations, Chinese ...  None   \n",
      "20  What do Chinese travelers want? Luxury 'star-r...  None   \n",
      "21  These are the best hotels in the U.S. and Euro...  None   \n",
      "22  Luxury brand Six Senses is opening its first h...  None   \n",
      "23  Charlie Munger: Cryptocurrency is 'stupid gamb...  None   \n",
      "24  How a real estate broker with a tennis side hu...  None   \n",
      "25  'Not too devastated': Why some laid-off worker...  None   \n",
      "26  Minimum wages are going up, but typical worker...  None   \n",
      "27  Why some young people are opting for 'human co...  None   \n",
      "\n",
      "                                                 Link  \n",
      "0   https://www.cnbc.com/2023/02/16/stocks-moving-...  \n",
      "1   https://www.cnbc.com/2023/02/16/stock-market-t...  \n",
      "2   https://www.cnbc.com/2023/02/15/stock-market-t...  \n",
      "3   https://www.cnbc.com/2023/01/27/india-is-a-tru...  \n",
      "4   https://www.cnbc.com/2023/01/20/belarus-sviatl...  \n",
      "5   https://www.cnbc.com/2023/01/20/i-wouldnt-chan...  \n",
      "6   https://www.cnbc.com/2023/01/20/covid-scale-pr...  \n",
      "7   https://www.cnbc.com/2023/01/20/summers-greate...  \n",
      "8   https://www.cnbc.com/2023/02/09/oil-shell-boar...  \n",
      "9   https://www.cnbc.com/2023/02/07/private-jet-us...  \n",
      "10  https://www.cnbc.com/2023/02/03/why-pink-hydro...  \n",
      "11  https://www.cnbc.com/2023/02/02/greenwashing-i...  \n",
      "12  https://www.cnbc.com/2023/02/01/concerns-over-...  \n",
      "13  https://www.cnbc.com/2023/02/16/supreme-court-...  \n",
      "14  https://www.cnbc.com/2023/02/16/china-spy-ball...  \n",
      "15  https://www.cnbc.com/2023/02/15/opioid-overdos...  \n",
      "16  https://www.cnbc.com/2023/02/15/moderna-ceo-ba...  \n",
      "17  https://www.cnbc.com/2023/02/15/watch-live-bid...  \n",
      "18  https://www.cnbc.com/2023/02/15/what-are-the-b...  \n",
      "19  https://www.cnbc.com/2023/02/14/where-are-chin...  \n",
      "20  https://www.cnbc.com/2023/02/10/chinese-travel...  \n",
      "21  https://www.cnbc.com/2023/02/09/what-are-the-b...  \n",
      "22  https://www.cnbc.com/2023/02/08/six-senses-is-...  \n",
      "23  https://www.cnbc.com/2023/02/16/billionaire-ch...  \n",
      "24  https://www.cnbc.com/2023/02/16/real-estate-br...  \n",
      "25  https://www.cnbc.com/2023/02/16/why-some-young...  \n",
      "26  https://www.cnbc.com/2023/02/16/minimum-wage-m...  \n",
      "27  https://www.cnbc.com/2023/02/16/what-is-human-...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "articles = soup.find_all('div', class_='Card-titleContainer')\n",
    "headlines = [article.find('a').text.strip() for article in articles]\n",
    "\n",
    "times = []\n",
    "links = []\n",
    "for article in articles:\n",
    "    try:\n",
    "        time = article.find('time')['datetime']\n",
    "    except (KeyError, TypeError):\n",
    "        time = None\n",
    "    try:\n",
    "        link = article.find('a')['href']\n",
    "    except (KeyError, TypeError):\n",
    "        link = None\n",
    "    times.append(time)\n",
    "    links.append(link)\n",
    "\n",
    "df = pd.DataFrame({'Headline': headlines, 'Time': times, 'Link': links})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6adc3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07ae64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "deb2bdeb",
   "metadata": {},
   "source": [
    "# question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dfd899d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "articles = soup.find_all(\"div\", class_=\"pod-listing-header\")\n",
    "\n",
    "data = []\n",
    "for article in articles:\n",
    "    title = article.find(\"a\").text\n",
    "    authors = article.find(\"div\", class_=\"text-xs\").text.strip()\n",
    "    date = article.find(\"div\", class_=\"text-xs\").find_next(\"div\", class_=\"text-xs\").text.strip()\n",
    "    link = article.find(\"a\")[\"href\"]\n",
    "    data.append((title, authors, date, link))\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Paper Title\", \"Authors\", \"Published Date\", \"Paper URL\"])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c61fe26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0598d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276e7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "208bef7c",
   "metadata": {},
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8c480a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Name Cuisine Location Rating Image URL\n",
      "0                                       \n",
      "1                                       \n",
      "2                                       \n",
      "3                                       \n",
      "4                                       \n",
      "5                                       \n",
      "6                                       \n",
      "7                                       \n",
      "8                                       \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Make a request\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants/buffet-special'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the restaurant cards\n",
    "restaurant_cards = soup.find_all('div', class_='restnt-card')\n",
    "\n",
    "# Initialize lists to store the extracted details\n",
    "names = []\n",
    "cuisines = []\n",
    "locations = []\n",
    "ratings = []\n",
    "image_urls = []\n",
    "\n",
    "# Loop through each restaurant card and extract the required details\n",
    "for card in restaurant_cards:\n",
    "    name = card.find('h3', class_='restnt-name ellipsis')\n",
    "    names.append(name.text if name else '')\n",
    "    \n",
    "    cuisine = card.find('p', class_='restnt-cuisine ellipsis')\n",
    "    cuisines.append(cuisine.text if cuisine else '')\n",
    "    \n",
    "    location = card.find('p', class_='restnt-loc ellipsis')\n",
    "    locations.append(location.text if location else '')\n",
    "    \n",
    "    rating = card.find('span', class_='restnt-rating')\n",
    "    ratings.append(rating.text if rating else '')\n",
    "    \n",
    "    image_wrapper = card.find('div', class_='img-wrapper')\n",
    "    image_url = image_wrapper.find('img')['src'] if image_wrapper else ''\n",
    "    image_urls.append(image_url)\n",
    "\n",
    "# Create a DataFrame with the extracted details\n",
    "df = pd.DataFrame({\n",
    "    'Name': names,\n",
    "    'Cuisine': cuisines,\n",
    "    'Location': locations,\n",
    "    'Rating': ratings,\n",
    "    'Image URL': image_urls\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2441128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3db68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63ddba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2161a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d587c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7279059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55de26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd32a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bbe441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d0ba61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d937e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305b6ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e1e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec8016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b18e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271caf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
